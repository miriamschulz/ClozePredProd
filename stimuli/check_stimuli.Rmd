---
title: "Check noun repetitions, continuations and question distributions"
author: "Miriam Schulz"
date: "2024-11-27"
output:
  html_document:
    number_sections: true
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# About

This script reads in a `csv` stimuli file for the Cloze Prediction & Production experiment as well as the fillers file
and checks for overlap of the nouns within and across both sets.


# Duplicate checks

## Check if there are duplicate nouns in the items (context and target nouns)

In a first step, the data is read in and the chosen items (indicated by a $1$ in the column `picked`) are subsetted:

```{r, message=FALSE}
library("tidyverse")
rm(list = ls())
```

```{r cars}
# Read in the data
dat <- read.csv("stim_exp.csv", header=TRUE)

# Filter the data set for the chosen candidate items
dat <- filter(dat, candidates=="1")

# Print items that do not occur exactly twice
subset(as.data.frame(xtabs(~ item_id + candidates, dat)), Freq != 2)
```

Write a function that will check if a word appears more than once,
either in the context nouns, in the target nouns, or across context and target nouns:

```{r}
check_overlap <- function(data, col1, col2) {
  # Check duplicates within each column
  duplicates_col1_table <- sort(table(data[[col1]])[table(data[[col1]]) > 2], decreasing=TRUE) / 2
  duplicates_col1 <- names(duplicates_col1_table)
  print("Duplicates within the context nouns:")
  print(duplicates_col1_table)
  
  duplicates_col2_table <- sort(table(data[[col2]])[table(data[[col2]]) > 1], decreasing=TRUE)
  duplicates_col2 <- names(duplicates_col2_table)
  print("Duplicates in the target nouns:")
  print(duplicates_col2_table)
  
  # Check for words appearing in both columns
  #duplicates_all_table <- (table(data[[col1]]) / 2) + (table(data[[col2]]))
  table1 <- table(data[[col1]]) / 2
  table2 <- table(data[[col2]])
  all_names <- union(names(table1), names(table2))
  aligned_table1 <- table1[all_names]
  aligned_table2 <- table2[all_names]
  aligned_table1[is.na(aligned_table1)] <- 0
  aligned_table2[is.na(aligned_table2)] <- 0
  duplicates_all_table <- sort(aligned_table1 + aligned_table2, decreasing=TRUE)
  duplicates_all_table <- duplicates_all_table[!is.na(names(duplicates_all_table))]

  print("Duplicates across both context and target nouns:")
  #print(length(duplicates_all_table))
  #print(sum(duplicates_all_table)) # 80 context nouns + N=4 overlapping nouns
  print(duplicates_all_table)
  
  common_words <- intersect(data[[col1]], data[[col2]])
  
  # Return results as a list
  return(list(
    duplicates_in_col1 = duplicates_col1,
    duplicates_in_col2 = duplicates_col2,
    common_words = common_words
  ))
}
```

Print the duplicate nouns:

```{r}
overlap <- check_overlap(dat, "W3", "target")
overlap
```


## Detect if a noun is still free to use in the fillers

Check if a noun has been used as context or target noun in the items, or if it is still free to use as a noun in the fillers:

```{r}
# Create a set of all nouns used as context or target noun in the items:
noun_set <- sort(unique(c(dat$W3, dat$target)))
# Check if a specific word is present in this set (TRUE/FALSE):
"mother" %in% noun_set
"work" %in% noun_set
```


## Check the fillers for overlap with the items

Once the fillers have been written, we should check again if there are any nouns in the final filler set that occur as context/target noun in the items.

Read in the file `fillers.csv` and write a function to check for overlap with the item noun set:

```{r}
# Read in the fillers
fillers <- read.csv("fillers.csv", header=TRUE)
#fillers <- filter(fillers, Chosen != "No")

# Merge the context sentence and target columns into a single vector
filler_sentences <- c(fillers$Sentence, fillers$Target)

# Split the sentences into individual words
filler_sentences <- gsub("[[:punct:]]", "", filler_sentences)
filler_words <- unique(tolower(unlist(strsplit(filler_sentences, "\\s+"))))
filler_words <- filler_words[filler_words != ""]  # remove empty string
#head(filler_words)
```

```{r}
check_filler_overlap <- function(item_nouns, filler_words) {
  for (w in filler_words) {
    if (w %in% item_nouns) {
      print(w)
    }
  }
}

check_filler_overlap(noun_set, filler_words)
```


# Continuations

## Check the continuations (spillover regions)

How often is each continuation (spillover word 1 + spillover word 2) used?

```{r}
spill <- select(dat, Spill1, Spill2)

# Check both spillover words as combination
spill$Spillover <- paste(spill$Spill1, spill$Spill2, sep = " ")
spillover_table <- data.frame(sort(xtabs(~ Spillover, spill) / 2, decreasing = TRUE))
write.csv(spillover_table, "spillover_counts.csv", row.names=FALSE)

# Check first spillover word only
spill1_table <- data.frame(sort(xtabs(~ Spill1, spill) / 2, decreasing = TRUE))
write.csv(spill1_table, "spillover_counts_spill1.csv", row.names=FALSE)
```


# Filler conditions

```{r}
# Filler conditions
xtabs(~ FillerType, fillers)

# Unexpected fillers
xtabs(~ Unexpected, filter(fillers, FillerType %in% c("Short", "StructureVariation")))

# Unexpectedness in both filler conditions (but not equally often,
# because there are fewer options for short unexpected fillers)
xtabs(~ FillerType + Unexpected, filter(fillers, FillerType %in% c("Short", "StructureVariation")))
prop.table(xtabs(~ FillerType + Unexpected, filter(fillers, FillerType %in% c("Short", "StructureVariation"))), margin=1)
```


# Comprehension questions

## Check the comprehension questions in the items

```{r}
dat$Sentence <- paste(dat$W1, dat$W2, dat$W3, dat$W4, dat$W5, 
                      dat$LastWord, dat$target,
                      dat$Spill1, dat$Spill2, dat$Continuation, dat$End,
                      sep = " ")
# Remove trailing whitespaces
dat <- dat %>% 
  mutate(Sentence = gsub("\\s{2,}", " ", Sentence))

questions <- select(dat, item_id, Sentence, HasQuestion, Question, Answer, QuestionAbout)

# Check that 1/3 of all items have a question:
length(unique(questions$item_id))  # N all items
xtabs(~ HasQuestion, questions) /2  # N items with question
prop.table(xtabs(~ HasQuestion, questions))  # % items with question

# Subset
questions <- filter(questions, HasQuestion == "Yes")

# Check that there is a question about each sentence position equally often
xtabs(~ QuestionAbout, questions) / 2

# Check that each sentence position has a true vs. false answer equally often
xtabs(~ Answer + QuestionAbout, questions) / 2
```

Inspect the actual questions:

```{r}
questions <- arrange(questions, QuestionAbout, Answer, item_id)
questions$HasQuestion <- NULL
#questions$Answer <- ifelse(questions$Answer == "D", "False", "True")
write.csv(questions, "comprehension_questions.csv", row.names=FALSE)
```


## Check the comprehension questions in the fillers

```{r}
fillers <- filter(fillers, !(FillerType %in% c("prac_comprehension", "prac_production")))

questions_fillers <- select(fillers, Filler_id, FillerType, Unexpected, Sentence, Target,
                            HasQuestion, Question, Answer, QuestionAbout)

# Check that 1/3 of all items have a question:
length(unique(questions_fillers$Filler_id))  # N all fillers
xtabs(~ HasQuestion, questions_fillers)  # N fillers with question
prop.table(xtabs(~ HasQuestion, questions_fillers))  # % fillers with question

# Subset
questions_fillers <- filter(questions_fillers, HasQuestion == "Yes")

# Check that each filler type has a question ~ equally often
xtabs(~ FillerType, questions_fillers)

# Check that there are ~ equally many questions per expected/unespectedness
xtabs(~ Unexpected, questions_fillers)

# Less important, but avoid any strong imbalance here:
xtabs(~ FillerType + Unexpected, questions_fillers)

# Check that there is a question about each sentence position ~ equally often
xtabs(~ QuestionAbout, questions_fillers)

# Check that each answer occurs equally often:
xtabs(~ Answer, questions_fillers)
xtabs(~ Answer + Unexpected, questions_fillers)
xtabs(~ Answer + FillerType, questions_fillers)
xtabs(~ Answer + QuestionAbout, questions_fillers)
```


# Pre-cloze context sentence length

## Context sentence length in the items

```{r}
dat$Sentence <- paste(dat$W1, dat$W2, dat$W3, dat$W4, dat$W5, 
                      dat$LastWord, dat$target,
                      dat$Spill1, dat$Spill2, dat$Continuation,# dat$End,
                      sep = " ")
# Remove trailing whitespaces
dat <- dat %>% 
  mutate(Sentence = gsub("\\s{2,}", " ", Sentence))

range(sapply(str_split(dat$Sentence, " "), length))
mean(sapply(str_split(dat$Sentence, " "), length))
```

## Context sentence length in the fillers

```{r}
#range(sapply(str_split(fillers$Sentence, " "), length))
#mean(sapply(str_split(fillers$Sentence, " "), length))
fillers$ContextLength <- sapply(str_split(fillers$Sentence, " "), length)
range(fillers$ContextLength)
mean(fillers$ContextLength)
```

Split into short vs. structure variation fillers:

```{r}
short <- filter(fillers, FillerType == "Short")
range(short$ContextLength)
mean(short$ContextLength)

variation <- filter(fillers, FillerType == "StructureVariation")
range(variation$ContextLength)
mean(variation$ContextLength)
```


# Latin square distribution

Some targets appear multiple times in the stimuli.

Targets that appear 2x in the stimuli must be put in separate lists.

The three targets that appear 3x each must be put in different lists but will inevitably occur twice in one list ('game', 'house', 'book').

```{r}
# The key constraints may specify an unequal distribution
xtabs(~ cond + Latin_constraints, dat)

# However, the full Latin square must be fully balanced:
xtabs(~ cond + Latin_list, dat)

# Each item must appear only once in each list, and not 0x in one and 2x in the
# other, so the range should be from 1 to 1:
range(xtabs(~ item_id + cond, dat))

# Each target must appear only once per list, with the exception of 
# game, house and book:
xtabs(~ target + Latin_list, dat)
target_xtabs <- as.data.frame(xtabs(~ target + Latin_list, dat))
target_xtabs[order(target_xtabs$Freq, target_xtabs$Latin_list), ]
```


# Division into blocks

## Items

Each list is divided into 4 blocks: one comprehension block and one production block, each divided in two to allow for a small break.

The distribution of items into blocks is used to keep some items apart, based on the following constraints:

- duplicate targets within a list must be kept maximally far apart (blocks 1 and 4)
- targets that also appear as a context noun must appear first (block 1) and be kept maximally far apart from the item with the same context noun (block 4)
- context nouns that appear several times must be placed maximally apart, into different blocks (if a noun appears 2x, blocks 1+4; if it appears 4x (max, 'girl'), once in each block)

The definition of these hard constrains was done manually on the Google docs.

The constraints will not be balanced:

```{r}
xtabs(~ Latin_list + Block_constraints, dat)
# xtabs(~ Block_constraints + cond + Latin_list, dat)
# xtabs(~ Block_constraints + HasQuestion + Latin_list, dat)
# xtabs(~ Block_constraints + Answer + Latin_list, dat)
```

But the final block distribution should be balanced with respect to the conditions and N questions in each block:

```{r}
xtabs(~ Latin_list + Block, dat, addNA=T)
xtabs(~ Block + cond + Latin_list, dat)

# xtabs(~ Block + HasQuestion + Latin_list, dat)
xtabs(~ Block + HasQuestion, dat) / 2
xtabs(~ Block + Answer, dat) / 2
xtabs(~ Block + QuestionAbout, dat) / 2
```

Also check that the division of target and context nouns into blocks worked (in each Latin list, this should range from 0x to 1x max for each block):

```{r}
# Targets: 
range(xtabs(~ target + Block + Latin_list, dat))

# Context nouns:
range(xtabs(~ W3 + Block + Latin_list, dat))
```


## Fillers

Check the distribution of items AND fillers in each block.

Fillers are used to counterbalance: 

- The N of questions per block
- The N of each answer (TRUE/FALSE) per block
- The QuestionAbout per block: ask approximately equally ofen about each sentence part.
  
Also, there should be roughtly the same N of expected/unexpected and short/variation fillers in each block.

The distribution of fillers into blocks was done manually on the fillers Google doc.

```{r}
xtabs(~ Block, fillers)

# Pre-format the fillers to merge
fillers <- rename(fillers, item_id = Filler_id)
fillers$cond <- ifelse(fillers$Unexpected == "yes",
                       paste(fillers$FillerType, "Unexp", sep="_"),
                       paste(fillers$FillerType, "Exp", sep="_"))
fillers$QuestionAbout <- ifelse(fillers$QuestionAbout == "Adjective/Adverb",
                                "Adjective", fillers$QuestionAbout)
xtabs(~ cond, fillers)

# Duplicate for Latin lists
fillers <- rbind(fillers, fillers)
fillers$Latin_list <- rep(1:2, each = nrow(fillers)/2)

# Merge fillers and items
all <- rbind(select(dat, item_id, cond, Latin_list, Block, HasQuestion, QuestionAbout, Answer),
             select(fillers, item_id, cond, Latin_list, Block, HasQuestion, QuestionAbout, Answer))

# Check final distribution
xtabs(~ Block + Latin_list, all)
xtabs(~ cond + Latin_list, all)
xtabs(~ Block + cond + Latin_list, all)

# Now these should all be balanced (in the columns):
xtabs(~ Block + HasQuestion, all) / 2
xtabs(~ Block + Answer, all) / 2
xtabs(~ Block + QuestionAbout, all) / 2

# Check in depth: cond + block + question + list; 
# these should be balanced row-wise:
xtabs(~ cond + Block + Latin_list, all)
xtabs(~ cond + Block + Latin_list, filter(all, HasQuestion == "Yes"))
xtabs(~ cond + Block + Latin_list, filter(all, HasQuestion == ""))
```

